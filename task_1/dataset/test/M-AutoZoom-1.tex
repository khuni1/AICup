%Input
Let $x$ be the original input image, $y$ be the true label associated with it, and $f_{\theta}$ be the target model. The goal is to create an adversarial example $x^*$ that misleads the model by generating perturbations efficiently through a zooming mechanism.

%Output
The output of the AutoZoom attack is an adversarial example $x^*$ that is generated by optimizing perturbations while ensuring the perturbations are perceptually relevant.

%Formula
The AutoZoom adversarial attack can be formulated as follows:
1. Initialize the input:
   $x^{(0)} = x$
2. Set parameters for perturbation and zooming:
   - Define the step size $\alpha$ and the maximum perturbation size $\epsilon$.
   - Initialize a zoom factor $\gamma$.
3. For each iteration $n = 1$ to $N$:
   - Generate a perturbation:
   $\delta_n = \alpha \cdot \text{sign}(\nabla_x L(f_{\theta}(x^{(n-1)}), y))$
   where $L$ is the loss function.
   - Apply the zooming mechanismM
   $x^{(n)} = \text{Clip}_{\mathcal{X}}(x^{(n-1)} + \delta_n \cdot \gamma)$
   ensuring:
   $\|x^{(n)} - x\|_p \leq \epsilon$

4. The final adversarial example is:
   $x^* = x^{(N)}$

%Explanation
The AutoZoom adversarial attack is designed to create effective adversarial examples through an innovative zooming mechanism that allows for the efficient generation of perceptually relevant perturbations. By leveraging gradients from the target model, the attack iteratively refines the perturbations while adjusting the "zoom" factor to focus on specific areas of the input image. This method enables the attacker to generate perturbations that are less likely to be detected by adversarial training methods and maintain a high level of effectiveness against various models. The resulting adversarial example $x^*$ can effectively deceive the target model, highlighting the need for robust defenses against sophisticated attack strategies like AutoZoom.
