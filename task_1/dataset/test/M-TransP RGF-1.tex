%Input
Let $x$ be the original input image, $y$ be the true label associated with it, and $f_{\theta}$ be the target model. The goal is to create a transferable adversarial example $x^*$ that can deceive multiple models.

%Output
The output of the Trans-P-RGF attack is a transferable adversarial example $x^*$ generated through randomized perturbations.

%Formula
The Trans-P-RGF adversarial attack can be formulated as follows:
1. Initialize the input:
   $x^{(0)} = x$
2. Set parameters for the optimization process:
   - Define the maximum perturbation size $\epsilon$, the number of iterations $N$, and the number of target models $M$.
3. For each iteration $n = 1$ to $N$:
   - Generate a random perturbation:
   $\delta_n = \text{Random}(-\epsilon, \epsilon)$
   ensuring the perturbation remains within the allowed range.
   - Create the perturbed input:
   $x^{(n)} = x^{(n-1)} + \delta_n$
   - Evaluate on multiple models:
   $\text{for each model } f_{\theta_j}, \text{ where } j = 1, \ldots, M$:
   - Check the model's prediction:
   $\text{if } f_{\theta_j}(x^{(n)}) \neq y, \text{ then accept } x^{(n)} \text{ as } x^*$

4. The final transferable adversarial example is:
   $x^* = x^{(n)} \text{ (if found)}$

%Explanation
The Trans-P-RGF adversarial attack enhances the traditional RGF approach by focusing on creating transferable adversarial examples that can successfully mislead multiple models. By incorporating random perturbations within a specified budget, the attack seeks to generate inputs that not only deceive the target model but also have a high chance of transferring to other models. This method leverages the inherent vulnerabilities of different architectures, demonstrating the potential for adversarial examples to be effective across varied systems. The resulting adversarial example $x^*$ highlights the risks associated with deploying machine learning models in real-world scenarios, where adversaries can exploit such transferable perturbations.
