%Input
Let $x$ be the original input image, $y$ be the true label associated with it, $\epsilon$ be the maximum allowable perturbation, and $\alpha$ be the step size. Additionally, let $M$ be a mask that guides the perturbation to specific regions of the input. The goal is to create an adversarial example $x^*$ that misleads the model while adhering to the mask.

%Output
The output is an adversarial example $x^*$ that misleads the model while ensuring that the perturbation is constrained to the masked regions defined by $M$. The generated adversarial example adheres to the perturbation constraints and leverages meta-learning to refine the attack strategy over iterations.


%Formula
$\delta^{(0)} = 0$

for $n = 1 \text{ to } N$:
    $\delta^{(n)} = \text{Clip}_{\mathcal{X}} \left( \delta^{(n-1)} + \alpha \cdot \text{sign} \left( \nabla_x L(f_\theta(x^{(n-1)}), y) \odot M \right) + \beta \cdot \text{meta}(\nabla_x L(f_\theta(x^{(n-1)}), y), M) \right)$

where:
- $L$ is the loss function,
- $f_\theta(x^{(n-1)})$ is the model's output for the input $x^{(n-1)}$,
- $\odot$ denotes the element-wise multiplication,
- $M$ is a binary mask guiding where perturbations can be applied,
- $\text{Clip}_{\mathcal{X}}(\cdot)$ projects the perturbed image back into the allowable input space.
- $\beta \cdot \text{meta}(\nabla_x L(f_\theta(x^{(n-1)}), y), M)$ incorporates a meta-learning component that learns to adaptively adjust the perturbation strategy based on the input's region.

%Explanation
The Meta-Masked PGD attack is an enhanced iterative method for generating adversarial examples, focusing on specific regions of the input image as defined by the mask $M$. By incorporating a meta-learning component, this variant adapts its perturbation strategy to better fit diverse inputs, making it more effective and stealthy. The addition of a meta-learning component enables the attack to learn from experience and improve over time, further increasing its effectiveness in evading defenses.