%Input
\[
\begin{aligned}
\mathbf{A} &: \text{Original adjacency matrix of the input graph.} \\
\hat{\mathbf{A}}^{(0)} &= \mathbf{A} \quad \text{(Initialization of the perturbed matrix).} \\
\delta^{(0)} &= 0 \quad \text{(Initial perturbation).} \\
N &: \text{Number of iterations.} \\
\alpha &: \text{Step size for perturbation updates.} \\
\rho &: \text{Perturbation budget constraint.} \\
L_t(\hat{\mathbf{A}}) &: \text{Targeted loss function dependent on the modified adjacency matrix.} \\
f_\theta(x) &: \text{Model prediction function.} \\
\mathcal{X} &: \text{Valid input space for perturbations.}
\end{aligned}
\]

%Output
The output is a set of adversarial examples $x^*$ that can deceive the model across multiple inputs, generated by iteratively updating a perturbation $\delta$ using an indirect gradient-based optimization method.

%Formula
$\hat{\mathbf{A}}^{(0)} = \mathbf{A}$
for $n = 1$ to $N:$ 
$\mathbf{\nabla}_{\hat{\mathbf{A}}} L_t(\hat{\mathbf{A}}) = -2 \sum_{i<j} (\mathbf{A}_{ij} - \hat{\mathbf{A}}_{ij}) \mathrm{sgn}(|\mathbf{A}_{ij} - \hat{\mathbf{A}}_{ij}|)$
$\delta^{(n)} = \text{Clip}_{\mathcal{X}} (\delta^{(n-1)} + \alpha \cdot \text{sign}(\nabla_\delta L(f_\theta(x + \delta^{(n-1)}), y)))$
$\hat{\mathbf{A}}^* = \arg \max_{\hat{\mathbf{A}}} L_t(\hat{\mathbf{A}}) \quad \text{subject to} \quad \sum_{i<j} |\mathbf{A}_{ij} - \hat{\mathbf{A}}_{ij}| \leq \rho$

%Explanation
The Graph-Aware Universal Perturbation Attack (G-UPA) variant of the original M-MGA G attack that uses an indirect gradient-based optimization method to generate adversarial examples. The attack starts with the original matrix $\mathbf{A}$ and iteratively updates it using the negative gradient of the loss function, which is computed as the sum of the signs of the absolute differences between the elements of $\mathbf{A}$ and the estimated matrix $\hat{\mathbf{A}}$. The perturbation $\delta$ is updated at each iteration using a clipping function to ensure that it remains within the allowable limits. After $N$ iterations, the resulting perturbation $\delta^{(N)}$ is added to the original input image $x$ to create a universal adversarial example $x^*$. This approach exploits the model's vulnerabilities by crafting a single perturbation that can generalize across multiple instances, making it a powerful method for generating adversarial examples that are difficult to defend against.