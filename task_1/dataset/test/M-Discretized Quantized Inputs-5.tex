%Input
Let $x$ be the original input image, and $y$ be the true label associated with it. The goal is to create an adversarial example $x^*$ that misleads the model, particularly focusing on the impact of discretization.

%Output
The output of the Discretized Inputs Attack is an adversarial example $x^*$ that is generated by modifying the original input $x$ while accounting for discretization effects in the model.

%Formula
The Discretized Inputs Attack can be formulated as:
$x^* = \text{Quantize}(x + \delta)$

where:
- $\delta$ is the perturbation added to the input $x$,
- $\text{Quantize}(\cdot)$ is a function that discretizes the values of the perturbed input to fit the required input space (e.g., mapping pixel values to integers).

The objective caan be framed as:
$\min_{\delta} \; \|x^* - x\|_p \quad \text{subject to} \quad f_\theta(x^*) \neq y$
where:
- $\|\cdot\|_p$ is the norm measuring the distance between the original and perturbed images,
- $f_\theta(x^*)$ is the model's output for the adversarial example.

%Explanation
In Discretized Inputs Attacks, the focus is on creating adversarial examples while taking into account the discretization process inherent in many machine learning models. When perturbing the input image, the model's input space often requires discrete values (e.g., pixel values between 0 and 255). This attack method generates a perturbation $\delta$ that, when added to the original image $x$, is then quantized to produce $x^*$. The goal is to ensure that $x^*$ misclassifies while remaining visually similar to $x$. The attack exploits the differences between continuous and discrete domains, aiming to bypass defenses that rely on discretization by creating adversarial inputs that challenge the model's predictions.
