%Input
Let $G = (V, E)$ be a graph where $V$ represents the set of nodes and $E$ represents the edges. Let $x \in \mathbb{R}^{|V| \times d}$ be the feature matrix of the nodes, $y$ be the true label associated with the graph, and $f_{\theta}(G)$ be the Graph Convolutional Network (GCN) model. The goal is to generate an adversarial example $G^*$ that misclassifies the input graph.

%Output
The output of the GCN-Node Attack is a modified graph $G^*$ that successfully misclassifies the original input while maintaining structural integrity.

%Formula
1. Initialize the graph:
   $G^{(0)} = G$
2. Set parameters for the optimization process:
   - Define the maximum perturbation size $\epsilon$ and the number of iterations $N$.
3. For each iteration $n = 1$ to $N$:
   - Compute the model's prediction:
   $\hat{y}^{(n)} = f_{\theta}(G^{(n-1)})$
   - Calculate the gradient of the loss function with respect to the node features:
   $g_n = \nabla_x L(f_{\theta}(G^{(n-1)}), y)$
   - Update the node features using the calculated gradient:
   $x^{(n)} = x^{(n-1)} + \delta_n$
   where:
   $\delta_n = -\alpha \cdot \text{sign}(g_n)$
   - Ensure the perturbation stays within the allowed range:
   $x^{(n)} = \text{Clip}_{\mathcal{X}}(x^{(n)})$
   ensuring:
   $\|x^{(n)} - x\|_p \leq \epsilon$
4. The final adversarial graph is:
   $G^* = G^{(N)}$

%Explanation
This variant introduces a modification to the original GCN-Node Attack by incorporating a new perturbation strategy that focuses on the structural integrity of the graph. 
The key difference between this variant and the main perturbation core lies in the update rule for the node features, which now incorporates an additional term that ensures the graph remains connected during the adversarial training process. This modification aims to create more realistic and robust adversarial examples that can better generalize across different graphs.