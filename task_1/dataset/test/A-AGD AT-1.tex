%Input
 $x^{(0)} = x$

%Output
The output of the AGD-AT attack is an adversarial example $x^*$ that effectively misclassifies the input while adhering to the constraints of adversarial training.

%Formula
1. Initialize the input:
   $
   x^{(0)} = x.
   $
2. Set parameters for the optimization process:
   - Define the maximum perturbation size $\epsilon$, the number of iterations $N$, and the step size $\alpha$.
3. For each iteration $n = 1$ to $N$:
   - Compute the model's prediction:
   $
   \hat{y}^{(n)} = f_{\theta}(x^{(n-1)}).
   $
   - Calculate the gradient of the loss function:
   $
   g_n = \nabla_x L(f_{\theta}(x^{(n-1)}), y).
   $
   - Update the input by adding the perturbation:
   $
   x^{(n)} = x^{(n-1)} + \alpha \cdot \text{sign}(g_n).
   $
   - Project the updated input back into the feasible region:
   $
   x^{(n)} = \text{Clip}_{\mathcal{X}}(x^{(n)}, \epsilon),
   $
   ensuring:
   $
   \|x^{(n)} - x\|_p \leq \epsilon.
   $

4. The final adversarial example is:
   $
   x^* = x^{(N)}.
   $

%Explanation
The AGD-AT (Adaptive Gradient Descent for Adversarial Training) attack generates adversarial examples through an iterative process that combines the principles of gradient descent and adaptive learning. By adjusting the input based on the gradients of the loss function, the attack aims to maximize the model's loss while maintaining the perturbation within a predefined limit. The resulting adversarial example $x^*$ effectively deceives the model, highlighting the vulnerabilities of models under adversarial training scenarios. This variant differs from the main PGD Fast-Universal attack in its use of adaptive gradient descent instead of projected gradient descent, which allows for more efficient and targeted attacks.