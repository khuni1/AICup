%Input
$ \epsilon $: Perturbation bound. \\
$ F_\theta $: Classifier with parameters $ \theta $. \\
$ X_b $: Batch of input data. \\
$ Y_b $: Batch of corresponding labels. \\
$ \nabla_{X_b} L(F_\theta(X_b), Y_b) $: Gradient of the loss with respect to $ X_b $. \\

%Output
Trained classifier $ F_\theta $ that is robust to adversarial perturbations within the bound $ \epsilon $. \\

%Formula
\begin{enumerate}
        \item For each batch $(X_b, Y_b)$ in the dataset $B$, repeat the following steps:
        \begin{enumerate}
            \item Calculate the adversarial perturbation $r$:
            \[
            r \leftarrow \epsilon \cdot \text{sign}\left( \nabla_{X_b} L(F_\theta(X_b), Y_b) \right)
            \]
            \item Calculate the gradient of the loss with respect to the updated batch $(X_b + r)$:
            \[
            \text{gradient} \leftarrow \nabla_{\theta} L(F_\theta(X_b + r), Y_b)
            \]
            \item Update the model parameters $\theta$ using the optimizer and the computed gradient.
        \end{enumerate}
    \end{enumerate}

%Explanation
Step 1: For each batch $ (X_b, Y_b) $ of the training data, compute the adversarial perturbation $ r $ by taking the sign of the gradient of the loss with respect to $ X_b $. \\
Step 2: Calculate the gradient of the loss with respect to the model parameters $ \theta $ using the perturbed inputs $ X_b + r $. \\
Step 3: Update the model weights $ \theta $ using the computed gradient and an optimizer. \\
