%Input
Let $x$ be the original input image, $y$ be the true label associated with it, and $f_{\theta}$ be the target model. The objective is to create an adversarial example $x^*$ that misleads the model while optimizing for cosine similarity in the embedding space.

%Output
The output of the Embedding Cosine attack is an adversarial example $x^*$ that is generated by maximizing the cosine similarity between the embeddings of the original and perturbed inputs.

%Formula
The Embedding Cosine adversarial attack can be formulated as follows:
1. Initialize the input:
   $x^{(0)} = x$
2. Set parameters for the optimization process:
   - Define the maximum perturbation size $\epsilon$ and the learning rate $\alpha$.
3. For each iteration $n = 1$ to $N$:
   - Compute the embedding:
   $e_n = f_{\theta}(x^{(n-1)})$
   
   where $e_n$ is the embedding of the current input.
   - Calculate the gradient with respect to the cosine similarity loss:
   $g_n = \nabla_x \left(1 - \frac{e_n \cdot e_t}{\|e_n\| \|e_t\|}\right)$
   where $e_t$ is the embedding of the target class.
   - Update the input:
   $x^{(n)} = x^{(n-1)} - \alpha \cdot g_n$
   
   - Apply clipping to ensure the perturbation is within bounds:
   $x^{(n)} = \text{Clip}_{\mathcal{X}}(x^{(n)})$
   ensuring:
   $\|x^{(n)} - x\|_p \leq \epsilon$

4. The final adversarial example is:
   $x^* = x^{(N)}$

%Explanation
The Embedding Cosine adversarial attack focuses on generating adversarial examples by manipulating the cosine similarity between the embeddings of the original and perturbed inputs. By leveraging the model's embedding space, the attack aims to create perturbations that not only mislead the model but also maintain a high level of semantic similarity with the original input. Through iterative optimization, the attack adjusts the input to maximize the cosine similarity loss, thus producing an effective adversarial example $x^*$. This method highlights the vulnerabilities of models that rely on embedding representations, emphasizing the need for robust defenses against such targeted attacks.
