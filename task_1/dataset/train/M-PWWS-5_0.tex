%Input
Let \( x \) be the original input text, \( y \) be the true label, and \( f_{\theta} \) be the target model. The PWWS attack generates adversarial examples by substituting words in the input text with semantically similar alternatives based on a probabilistic model.

%Output
The output of the PWWS attack is a modified text input \( x^* \) that aims to mislead the model while preserving the overall meaning of the original text.

%Formula
The PWWS adversarial attack can be formulated as follows:
1. Initialize the input and true label:
   $
   (x, y).
   $
2. Identify the words in the input text:
   $
   x = (w_1, w_2, \ldots, w_n).
   $
3. For each word \( w_i \) in the input:
   - Generate a set of candidate substitutions \( S(w_i) \) based on a similarity measure:
   $
   S(w_i) = \{w_{i1}, w_{i2}, \ldots, w_{im}\},
   $
   where each \( w_{ij} \) is a candidate synonym for \( w_i \).
4. For each candidate substitution \( w_{ij} \):
   - Construct a modified input:
   $
   x' = (w_1, \ldots, w_{i-1}, w_{ij}, w_{i+1}, \ldots, w_n).
   $
   - Evaluate the model's prediction:
   $
   \hat{y} = f_{\theta}(x').
   $
   - If \( \hat{y} \neq y \), then \( x' \) is a candidate adversarial example.
5. The goal is to find:
   $
   x^* = \arg\max_{x'} \text{Prob}(f_{\theta}(x') \neq y).
   $

%Explanation
The Probability Weighted Word Saliency (PWWS) attack generates adversarial examples by probabilistically substituting words in the original text \( x \). By assessing the impact of different word substitutions on the model's prediction, the attack seeks to create a new input \( x^* \) that retains the original meaning but causes the model to misclassify. This approach highlights the vulnerabilities of natural language processing models to subtle perturbations in text and underscores the importance of developing robust defenses against such attacks.
