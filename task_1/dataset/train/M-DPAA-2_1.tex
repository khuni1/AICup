%Input
$ \mathbf{x} \in \mathbb{R}^n $ represents the original input image,  
$ \delta \in \mathbb{R}^n $ is the adversarial perturbation,  
$ D(\mathbf{x}, \mathbf{x} + \delta) $ is a distance metric,  
$ f(\mathbf{x} + \delta) $ is the classifier's decision function,  
$ c $ is a trade-off constant controlling the perturbation size,  
and the adversarial constraint is $ \mathbf{x} + \delta \in [0, 1]^n $.  

%Output
Adversarial perturbation $\delta^*$, which minimizes the distance between the original image $\mathbf{x}$ and the adversarial example $\mathbf{x} + \delta^*$ while ensuring that the classifier is misled. The adversarial example satisfies the constraint $\mathbf{x} + \delta \in [0, 1]^n$, where $\delta^*$ is the optimal perturbation found by balancing the trade-off between distortion and the success of the attack.

%Formula
$\min_{\delta} \quad D(\mathbf{x}, \mathbf{x} + \delta) + c \cdot f(\mathbf{x} + \delta) \quad \text{s.t.} \quad \mathbf{x} + \delta \in [0, 1]^n$

%Explanation
The reformulated objective introduces a penalty term based on the classifierâ€™s output. The constant $c$ balances the trade-off between minimizing the perturbation size and achieving a successful attack, ensuring that the adversarial example satisfies the constraint $f(\mathbf{x} + \delta) \leq 0$. This variant Distortion-Penalty Adversarial Attack (DPAA) is different from the main perturbation core in that it incorporates a distance metric-based approach with a penalty term for misclassification, making it more robust to various image transformations and classification boundaries.