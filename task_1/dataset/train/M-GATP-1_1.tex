%Input
Let $x$ be the original input, and let $y$ be the true class label. The adversary aims to generate an adversarial example $x^*$ that causes misclassification while ensuring the perturbation remains within a specified constraint.

Given:
- $f_{\theta}$: The target model with parameters $\theta$.
- $J(f_{\theta}(x), y)$: The loss function associated with the modelâ€™s prediction.
- $\epsilon$: The maximum allowable perturbation.
- $\mathcal{X}$: The valid input space.

The adversarial example $x^*$ is generated by modifying $x$ based on the gradient of the loss function, ensuring that the perturbation remains within the allowed bounds.


%Output
The output of the gradient-based maximized perturbation attack is an adversarial example $x^*$ generated by modifying the original input $x$ based on the gradients of the loss function. This can be expressed as:
$x^* = x + \epsilon \cdot \text{sign}(\nabla_x J(f_\theta(x), y))$

%Formula
$\text{maximize} \quad \text{score}(f_\theta(x^*), c) - \text{score}(f_\theta(x^*), i)$
for all classes $i \neq c$, where $c$ is the target class.

%Explanation
This Gradient-Aligned Targeted Perturbation (GATP) Attack  variant of the perturbation core differs from the score-based approach by directly utilizing the gradients of the loss function to update the input. Instead of maximizing the difference between target and non-target scores, this attack updates the input based on the sign of the gradient of the loss function with respect to the input. This can lead to a more efficient generation of adversarial examples that are targeted towards specific classes.