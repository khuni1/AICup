%Input
The input consists of the original input $x$, the perturbation $\delta$, the neural network model $\theta$, and the target class probability $\overline{y}$. 

%Formula
$\overline{y}_6 = \max_{y, \delta} \left( -L(\theta, x + \delta k \overline{y}) - \lambda_c C(\delta) - \lambda_{\text{tv}} TV(\delta) - \lambda_s \text{Dissim}(\delta) \right)$
Objective Function:
\begin{equation*}
\overline{y}_6
\end{equation*}
This represents the maximum value of the objective function, indicating the optimal perturbation \(\delta\) that maximizes the loss subject to the constraints.

The objective function for the Shadow Attack is given by:
\begin{equation*}
\overline{y}_6 = \max_{y, \delta} \left( -L(\theta, x + \delta k \overline{y}) - \lambda_c C(\delta) - \lambda_{\text{tv}} TV(\delta) - \lambda_s \text{Dissim}(\delta) \right)
\end{equation*}

where:
\begin{equation*}
\text{Loss Function:} \quad -L(\theta, x + \delta k \overline{y})
\end{equation*}

The loss function term calculates the negative of the loss $L$ for the perturbed input $x + \delta k \overline{y}$, which aims to misclassify the input sample.

Regularization Terms:
$\lambda_c C(\delta)$
$\lambda_{\text{tv}} TV(\delta)$
$\lambda_s \text{Dissim}(\delta)$


The regularization terms include:
$C(\delta)$
which represents a constraint on the perturbation magnitude.

$TV(\delta)$
which represents the total variation of the perturbation.
$\text{Dissim}(\delta)$which represents the dissimilarity of the perturbation to a reference pattern or distribution.

Hyperparameters
$\lambda_c$
$\lambda_{\text{tv}}$ 
$\lambda_s$

These are hyperparameters that control the relative importance of each regularization term in the objective function. They determine the trade-off between encouraging misclassification and enforcing perturbation constraints.

%Output
The output is the maximum value of the objective function $\overline{y}_6$, which represents the optimal perturbation $\delta$ that maximizes the loss subject to various constraints.

%Explanation
The Shadow Attack method aims to maximize the loss function and find the perturbation $\delta$ that maximizes the loss function for the neural network while also satisfying several regularization constraints. The negative loss term encourages the model to misclassify the perturbed input. Regularization terms are used to control the magnitude, smoothness, and dissimilarity of the perturbation to ensure it remains imperceptible and adheres to certain characteristics. The hyperparameters $\lambda_c$, $\lambda_{\text{tv}}$, and $\lambda_s$ balance these trade-offs by adjusting the weight of each regularization term in the objective function.
