%Input
Let $x$ be the original input image and $y$ be the true label associated with it. The objective is to create an adversarial example $x^*$ that misleads the model.
- $J(f_\theta(x), y)$ is a score function, often based on the logits output by the model,
- $f_\theta(x)$ is the model's prediction for input $x$,
- $\epsilon$ controls the perturbation magnitude.

%Output
The output of the score-based attack is an adversarial example $x^*$ generated by modifying the original input $x$ based on the scores (logits) provided by the model.

%Formula
The score-based attack updates the input by maximizing the difference between the target class score and the scores of other classes. This can be expressed as:
$x^* = x + \epsilon \cdot \text{sign}(\nabla_x J(f_\theta(x), y))$

In some formulations, the update can be defined as:
$\text{maximize} \quad \text{score}(f_\theta(x^*), c) - \text{score}(f_\theta(x^*), i)$
for all classes $i \neq c$, where $c$ is the target class.

% Explanation
In score-based attacks, the goal is to manipulate the input in such a way that the model's confidence in the target class $c$ increases, while the confidence in all other classes decreases. By using the scores (logits) from the model's output, the attack adjusts the input to create a perturbation that effectively changes the model's prediction. This method can exploit the model's structure and decision boundaries, leading to adversarial examples that are often harder to detect by traditional defenses.
