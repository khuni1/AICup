%Input
The input data is \(X\), and the true label is \(y_{\text{true}}\).

%Output
The output is the adversarial example \(X_{\text{adv}}\) and the total loss value.

%Formula
The Basic Iterative Method (BIM) for adversarial example generation involves iteratively updating the input \(X\) using the formula:
$X_{\text{adv}} = X + \epsilon \frac{\nabla_X J(X, y_{\text{true}})}{||\nabla_X J(X, y_{\text{true}})||_2}$
$\epsilon$ is the step size or perturbation magnitude.
$\nabla_X J(X, y_{\text{true}})$ is the gradient of the loss function $J$ with respect to the input $X$, evaluated at the true label $y_{\text{true}}$
$||\nabla_X J(X, y_{\text{true}})||_2$ \text{ denotes the } $L_2$ norm of the gradient, used to normalize the perturbation.

The loss function for BIM-A is given by:
\begin{equation*}
\text{Loss} = \frac{1}{{(m - k)} + \lambda_k} \left( \sum_{i \in \text{CLEAN}} L(X_i | y_i) + \lambda \sum_{i \in \text{ADV}} L(X_{\text{adv}_i} | y_i) \right)
\end{equation*}

where:
\begin{equation*}
m \text{ is the number of clean examples.}
\end{equation*}
\begin{equation*}
k \text{ is the number of adversarial examples.}
\end{equation*}
\begin{equation*}
\lambda_k \text{ and } \lambda \text{ are weighting factors for clean and adversarial examples.}
\end{equation*}
\begin{equation*}
L(X_i | y_i) \text{ is the loss for each clean example.}
\end{equation*}
\begin{equation*}
L(X_{\text{adv}_i} | y_i) \text{ is the loss for each adversarial example.}
\end{equation*}

%Explanation
The Basic Iterative Method B (BIM) is a variant of the Fast Gradient Sign Method (FGSM) that applies perturbations iteratively. The process involves updating the input by adding a small perturbation in the direction of the gradient of the loss function, scaled by the $L_2$ norm of the gradient. This iterative approach allows for more refined adversarial perturbations, which can lead to stronger adversarial examples.

The loss function for BIM-B combines the loss for clean examples and adversarial examples, weighted by factors $\lambda_k$ and $\lambda$. This combined loss helps to balance the model's performance on both clean and adversarial examples, improving its robustness against adversarial attacks.
