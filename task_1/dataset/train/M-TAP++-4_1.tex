%Input
The victim policy to be attacked is $\pi$.
Environment where the victim policy operates is $E$.
Adversarial policy to train is $\pi_{\text{adv}}$.
Total number of time steps in each episode is $T$.
Loss function capturing the adversarial objective is $\mathcal{L}(\pi_{\text{adv}}, \pi)$.
Learning rate for adversarial policy training is $\eta$.

%Output
Output: Trained adversarial policy $\pi_{\text{adv}}$.


%Formula
1.Adversarial Policy Training:
   \[
   \pi_{\text{adv}}^* = \arg \max_{\pi_{\text{adv}}} \mathbb{E}_{\tau \sim E(\pi, \pi_{\text{adv}})} \Big[ R_{\text{adv}}(\tau) \Big],
   \]
   where $\tau = (\mathbf{s}_1, \mathbf{a}_1, \dots, \mathbf{s}_T, \mathbf{a}_T)$ is a trajectory generated by interacting policies $\pi$ and $\pi_{\text{adv}}$, and $R_{\text{adv}}$ is the adversarial reward.

2.Gradient Update:
   - Using gradient-based reinforcement learning, update $\pi_{\text{adv}}$:
     \[
     \pi_{\text{adv}} \leftarrow \pi_{\text{adv}} + \eta \nabla_{\pi_{\text{adv}}} \mathcal{L}(\pi_{\text{adv}}, \pi),
     \]
     where $\mathcal{L}$ is typically derived from the adversarial objective.

3.Loss Function:
   \[
   \mathcal{L}(\pi_{\text{adv}}, \pi) = -\mathbb{E}_{\tau \sim E(\pi, \pi_{\text{adv}})} \Big[ R_{\text{adv}}(\tau) \Big].
   \]

%Explanation
The variant proposed Training Adversarial Policies with Perturbation and Reward (TAP++). TAP++ builds upon the original TAP method by incorporating a perturbation strategy inspired by the PGD Fast-Universal attack. The new approach adds a noise term to the adversarial reward $R_{\text{adv}}$ to make it more robust against the victim policy's defenses. This modification is achieved by adding a small Gaussian noise $\epsilon \sim \mathcal{N}(0, \sigma^2)$ to the reward function, resulting in a new loss function $\mathcal{L}(\pi_{\text{adv}}, \pi) = -\mathbb{E}_{\tau \sim E(\pi, \pi_{\text{adv}})} \Big[ R_{\text{adv}}(\tau) + \epsilon \Big]$. The learning rate $\eta$ is also adjusted to accommodate the added noise term. TAP++ aims to improve the robustness and effectiveness of the adversarial policy by incorporating a perturbation strategy that mimics the PGD Fast-Universal attack, while maintaining the core principles of the original TAP method.