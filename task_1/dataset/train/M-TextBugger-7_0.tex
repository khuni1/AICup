%Input
Let \( x \) be the original input text, \( y \) be the true label, and \( f_{\theta} \) be the target model. The TextBugger attack generates adversarial examples by introducing minimal changes to the input text.

%Output
The output of the TextBugger attack is a modified text input \( x^* \) that aims to mislead the model while preserving the original meaning of the text.


%Formula
The TextBugger adversarial attack can be formulated as follows:
1. Initialize the input text and true label:
   $
   (x, y).
   $
2. Identify the words in the input text:
   $
   x = (w_1, w_2, \ldots, w_n).
   $
3. For each word \( w_i \) in the input:
   - Generate a set of candidate modifications \( S(w_i) \) based on a similarity measure, such as character-level perturbations:
   $
   S(w_i) = \{w_{i1}, w_{i2}, \ldots, w_{im}\}.
   $
4. For each candidate substitution \( w_{ij} \):
   - Construct a modified input:
   $
   x' = (w_1, \ldots, w_{i-1}, w_{ij}, w_{i+1}, \ldots, w_n).
   $
   - Evaluate the model's prediction:
   $
   \hat{y} = f_{\theta}(x').
   $
   - If \( \hat{y} \neq y \), then \( x' \) is a candidate adversarial example.
5. The goal is to find:
   $
   x^* = \arg\max_{x'} \text{Prob}(f_{\theta}(x') \neq y),
   $
   ensuring minimal modification to the original text.

%Explanation
The TextBugger attack generates adversarial examples by making small, strategically chosen modifications to the input text \( x \). By evaluating the impact of these modifications on the model's predictions, the attack seeks to create a new input \( x^* \) that retains the original meaning while causing the model to misclassify. This approach illustrates the vulnerabilities of natural language processing models to subtle text perturbations and emphasizes the need for robust defenses against such adversarial techniques.
