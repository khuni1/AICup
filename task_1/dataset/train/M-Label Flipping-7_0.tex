%Input
\[
\begin{aligned}
\mathcal{D} & : \text{Dataset with samples } \{(\mathbf{x}_i, y_i)\}_{i=1}^N, \text{ where } y_i \in \{0, 1\}.
f(\mathbf{x}) & : \text{Target model to be trained on } \mathcal{D}.
p_{\text{flip}} & : \text{Proportion of labels to be flipped in the dataset.}
\end{aligned}
\]


%Output 
\[
\mathcal{D}_{\text{adv}} = \{(\mathbf{x}_i, y_i^{\text{adv}})\}_{i=1}^N, \text{ where } y_i^{\text{adv}} \neq y_i \text{ for a subset of flipped labels}.
\]

%Formula
1. Selection of Flip Targets:  
   Randomly select a subset $\mathcal{I}_{\text{flip}} \subseteq \{1, 2, \ldots, N\}$ such that:
   \[
   |\mathcal{I}_{\text{flip}}| = \lceil p_{\text{flip}} \cdot N \rceil.
   \]

2. Label Flipping:  
   For each $i \in \mathcal{I}_{\text{flip}}$, flip the label:
   \[
   y_i^{\text{adv}} = 1 - y_i.
   \]

3. Adversarial Dataset:  
   Construct the modified dataset:
   \[
   \mathcal{D}_{\text{adv}} = \{(\mathbf{x}_i, y_i^{\text{adv}}) : i \in \mathcal{I}_{\text{flip}}\} \cup \{(\mathbf{x}_i, y_i) : i \notin \mathcal{I}_{\text{flip}}\}.
   \]

%Explanation
The Label Flipping Adversarial Attack targets the training dataset by flipping the labels of a subset of data points, aiming to degrade the performance of the target model $f(\mathbf{x})$. 

1. Objective: Mislead the learning process by introducing incorrect labels into the dataset, causing the model to learn an incorrect mapping from inputs to outputs.

2. Controlled Proportion: A specified proportion $p_{\text{flip}}$ of labels is flipped, allowing control over the degree of poisoning in the dataset.

3. Mechanism: Label flipping is straightforward and non-gradient-based, as it directly alters the ground truth labels without requiring access to the model's gradients or internal structure.

4. Adversarial Impact: By training on the adversarial dataset $\mathcal{D}_{\text{adv}}$, the model's accuracy and generalization are significantly reduced, particularly if the flipped labels are strategically chosen.