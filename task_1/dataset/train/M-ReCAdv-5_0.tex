%Input
Let $x$ be the original input image, $y$ be the true label associated with it, and $\epsilon$ be the maximum allowed perturbation. The goal is to create an adversarial example $x^*$ that misleads the model while ensuring that the reconstruction of the perturbed image remains close to the original input.

%Output
The output of the ReCAdv attack is an adversarial example $x^*$ generated by optimizing a perturbation that allows for effective reconstruction while evading the model's classification.

%Formula
The ReCAdv attack can be formulated as:
$x^* = x + \delta$
where $\delta$ is the perturbation optimized to satisfy:
$\min_{\delta} \; \| \delta \|_p \quad \text{subject to} \quad f_\theta(x + \delta) \neq y \quad \text{and} \quad \| \text{Reconstruct}(x + \delta) - x \|_q \leq \tau$
where:
- $f_\theta(x + \delta)$ is the model's output for the perturbed input,
- $\| \cdot \|_p$ and $\| \cdot \|_q$ are norms measuring the distances,
- $\text{Reconstruct}(\cdot)$ is the reconstruction function that aims to revert the perturbed input to its original form,
- $\tau$ is a threshold controlling the allowable difference between the reconstructed image and the original.

%Explanation
The Reconstruction-Constrained Adversarial Attack (ReCAdv) attack focuses on creating adversarial examples that can effectively deceive a model while preserving the ability to reconstruct the original image from the perturbed input. This is achieved by optimizing a perturbation $\delta$ that not only misclassifies the perturbed input but also ensures that the reconstructed image remains close to the original input within a specified threshold $\tau$. By balancing the adversarial objective and the reconstruction constraint, ReCAdv is designed to generate effective adversarial examples that exploit model vulnerabilities while maintaining a level of perceptual fidelity to the original image.
