%Input
Original input: $X \in \mathbb{R}^d$
True label: $y \in \mathcal{Y}$
Loss function: $\mathcal{L}(f(X), y)$, where $f$ is the model
Perturbation budget: $\epsilon$
Step size: $\alpha$
Number of iterations: $T$
Decay factor: $\mu$

%Output
Adversarial example $X_{\text{adv}}$ such that $\|X_{\text{adv}} - X\|_\infty \leq \epsilon$.

%Formula
Initialize $X^{(0)}_{\text{adv}} = X$ and $g^{(0)} = 0$. For $t = 0, \dots, T-1$:
\[g^{(t+1)} = \mu \cdot g^{(t)} + \frac{\nabla_X \mathcal{L}(f(X^{(t)}_{\text{adv}}), y)}{\|\nabla_X \mathcal{L}(f(X^{(t)}_{\text{adv}}), y)\|_1}\]
\[X^{(t+1)}_{\text{adv}} = X^{(t)}_{\text{adv}} + \alpha \cdot \text{sign}(g^{(t+1)})\]
Project $X^{(t+1)}_{\text{adv}}$ onto the $\epsilon$-ball around $X$:
\[X^{(t+1)}_{\text{adv}} = \text{clip}(X^{(t+1)}_{\text{adv}}, X - \epsilon, X + \epsilon)\]

%Explanation
The M-Momentum variant of the attack introduces a momentum term $g^{(t)}$ that accumulates gradients across iterations with a decay factor $\mu$. This stabilizes the direction of optimization and prevents oscillations in the perturbation update. The use of a momentum term makes the attack more robust to noise in the loss function and improves its ability to find effective adversarial examples.

This variant is different from the M-Momentum Iterative FGSM (M-MIFGSM) method because it incorporates a momentum term, which helps to stabilize the direction of optimization and improve the effectiveness of the perturbation update. The M-Momentum variant is more resistant to noise in the loss function and can generate more robust adversarial examples than the original M-FGSM method.