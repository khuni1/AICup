%Input
The classifier network is denoted as $f$, which takes an input and provides a classification. The input point/image is represented by $x \in \mathbb{R}^d$. The image size is denoted by $w$, and $c$ represents the number of color channels in the image. The $\ell_p$-radius for perturbation is given by $\epsilon$. The true label of the input image is $y \in \{1, \ldots, K\}$. The number of iterations is $N$. The perturbed image that minimizes the classifier loss is $\hat{x} \in \mathbb{R}^d$. The loss function of the classifier for input $x$ and label $y$ is denoted by $L(f(x), y)$. The side length of the square to modify at iteration $i$ is $h(i)$. The perturbation added to the image is represented by $\delta$. The sampling distribution for perturbation $\delta$ is $P(\epsilon, h(i), w, c, x, \hat{x})$. The projection function that ensures the perturbed image remains within the $\ell_p$-radius and the valid pixel range is denoted by $\text{Project}(\cdot)$.


%Output
The output of the Subtle Adv Adversarial Attack is a perturbed input \(\tilde{x}\) that remains visually indistinguishable from \(x\) but is misclassified.

%Formula
1. Initialize the original input \( x \) and the perturbation \(\delta\) with a small magnitude:
   $\tilde{x} = x + \delta,$
   where $\|\delta\| \leq \epsilon.$
2. Define the objective function to minimize the loss while maintaining subtlety:
   $J(f(\tilde{x}), y) + \lambda \cdot \|\delta\|^2,$
   where $J$ is the loss function, $f$ is the model, and $\lambda$ is a regularization parameter that controls the trade-off between misclassification and perturbation size.
3. Use a gradient-based optimization algorithm (e.g., gradient descent) to iteratively update \(\delta\):
   $\delta \leftarrow \delta - \alpha \nabla_{\delta} \left(J(f(\tilde{x}), y) + \lambda \cdot \|\delta\|^2\right),$
   where $\alpha$ is the learning rate.
4. Update the perturbed input:
   $\tilde{x} = x + \delta.$

%Explanation
The StealthGrad Attack Optimization Method generates adversarial examples by carefully perturbing the original input \(x\) to create a new input \(\tilde{x}\). The objective is to achieve misclassification while ensuring that the perturbation remains small enough to keep \(\tilde{x}\) visually similar to \(x\). This variant incorporates a gradient-based optimization method, which updates the perturbation based on the gradient of the loss function. This approach aims to improve the stealthiness and effectiveness of the attack by leveraging the model's internal structure.