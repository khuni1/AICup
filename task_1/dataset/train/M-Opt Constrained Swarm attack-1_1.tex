%Input
Let $f_\theta$ be a neural network parameterized by $\theta$, and let $x$ be the clean input sample with the corresponding true label $y$. The adversarial perturbation $\delta$ is iteratively updated to maximize the loss function $L(f_\theta(x), y)$ while ensuring that the perturbation remains within a predefined constraint. The attack follows a constrained iterative approach where each step updates $\delta$ based on the gradient information while ensuring that it remains within the valid perturbation bounds.

Given:
- $f_\theta$: Neural network model parameterized by $\theta$.
- $x$: Clean input sample.
- $y$: True label corresponding to $x$.
- $\delta$: Adversarial perturbation applied to the input.
- $L(f_\theta(x), y)$: Loss function of the model.
- $\epsilon_n$: Constrained perturbation at iteration $n$.
- $\epsilon_{max}$: Maximum allowed perturbation.
- $\alpha$: Step size for perturbation updates.
- $\mathcal{X}$: Input space constraints.

The adversarial example is generated by iteratively updating $\delta$ while ensuring that it remains within the constraint $\mathcal{X}$. The perturbation starts with an initial constrained value $\delta^{(0)} = \epsilon_n$ and is iteratively refined using the gradient of the loss function.

%Output
$\delta^{(N)}$: Final adversarial perturbation after $N$ iterations. \\
$x_{adv} = x + \delta^{(N)}$: Adversarial example obtained by applying the optimized perturbation. \\
$\epsilon_n$: Constraint value dynamically adjusted during optimization. \\
$L(f_\theta(x_{adv}), y)$: Final loss value after attack execution.


%Formula
$\delta^{(0)} = 0$
for n = 1 to N:
$\delta^{(n)} = \text{Clip}_{\mathcal{X}} \left( \delta^{(n-1)} + \alpha \cdot \text{sign} \left( \nabla_\delta L(f_\theta(x + \delta^{(n-1)}), y) \right) \right)$
where $\delta^{(0)} = \epsilon_n$ is the constrained perturbation, with $\epsilon_n < ||\nabla f_\theta(x)||_\infty$
$\epsilon_n = \text{min}(\epsilon_{max}, \epsilon_n + \alpha \cdot \text{sign} \left( \nabla_\delta L(f_\theta(x), y) \right))$ 
where $\epsilon_{max}$ is the maximum allowed perturbation

%Explanation
The Opt-Constrained Swarm attack variant modifies the original swarm-based optimization method to incorporate a constraint on the perturbation. This variant ensures that the generated adversarial examples remain within a bounded region, making them more stealthy and difficult to detect. By introducing a constrained optimization framework, this variant improves upon the Practical Swarm attack by providing a more robust and controlled approach to generating adversarial examples.