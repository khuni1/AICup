%Input
$ \epsilon $: Perturbation bound. \\
$ F_\theta $: Classifier with parameters $ \theta $. \\
$ X_b $: Batch of input data. \\
$ Y_b $: Batch of corresponding labels. \\
$ \nabla_{X_b} L(F_\theta(X_b), Y_b) $: Gradient of the loss with respect to $ X_b $. \\

%Output
Trained classifier $ F_\theta $ that is robust to adversarial perturbations within the bound $ \epsilon $. \\

%Formula
\begin{enumerate}
        \item For each batch $(X_b, Y_b)$ in the dataset $B$, repeat the following steps:
        \begin{enumerate}
            \item Calculate the adversarial perturbation $r$:
            \[
            r \leftarrow \epsilon \cdot \text{sign}\left( \nabla_{X_b} L(F_\theta(X_b), Y_b) \right)
            \]
            \item Calculate the gradient of the loss with respect to the updated batch $(X_b + r)$:
            \[
            \text{gradient} \leftarrow \nabla_{\theta} L(F_\theta(X_b + r), Y_b)
            \]
            \item Update the model parameters $\theta$ using the optimizer and the computed gradient.
        \end{enumerate}
    \end{enumerate}

%Explanation
This variant of PGD adds Monte Carlo noise to the perturbation process, which introduces randomness and makes the attack more effective. The addition of noise improves the robustness of the adversarial examples generated by PGD.
This variant maintains the core principle of PGD but introduces an additional layer of randomness, making it more effective at generating adversarial examples.