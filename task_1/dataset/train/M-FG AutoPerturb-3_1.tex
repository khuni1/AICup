%Input
Let \( x \) be the original image input, \( y \) be the true label, and \( f_{\theta} \) be the target model. The goal of the Feature-Guided AutoPerturb (FG-AutoPerturb) attack is to generate an adversarial example \( x^* \) by applying perturbations guided by a feature importance score while ensuring effective misclassification.

Define:
- \( s \) as the feature importance score derived from gradient information.
- \( \gamma \) as the zoom factor that scales perturbations adaptively.
- \( \epsilon \) as the perturbation budget controlling the attack strength.


%Output
The output of the AutoPerturb adversarial attack is an adversarial example $x^*$ generated by optimizing perturbations while ensuring the perturbations are directed towards specific features in the input image.

%Formula
1. Initialize the input:
   $x^{(0)} = x$
2. Set parameters for perturbation strategy and zooming:
   - Define a feature importance score $s$ based on the gradient of the loss function.
   - Initialize a zoom factor $\gamma$.
3. For each iteration $n = 1$ to $N$:
   - Generate a perturbation with direction proportional to the feature importance score:
   $\delta_n = \alpha \cdot s_n \cdot \text{sign}(\nabla_x L(f_{\theta}(x^{(n-1)}), y))$
   where $s_n = \gamma^n$.
   - Apply the zooming mechanism
   $x^{(n)} = \text{Clip}_{\mathcal{X}}(x^{(n-1)} + \delta_n \cdot \gamma)$
   ensuring:
   $\|x^{(n)} - x\|_p \leq \epsilon$

%Explanation
The Feature-Guided AutoPerturb (FG-AutoPerturb) adversarial attack is a variant of the original AutoZoom attack that incorporates a feature importance score to guide the perturbation strategy. By directing the perturbations towards specific features in the input image, the attacker can potentially improve the stealthiness and effectiveness of the generated adversarial example. This approach leverages the concept of feature importance from machine learning model interpretability techniques to refine the perturbation mechanism, providing a more targeted attack strategy against models.