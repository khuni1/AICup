%Input
\[
\text{Let } x \text{ be the original input image,}
\]
\[
y \text{ be the true label,}
\]
\[
\delta \text{ be the adversarial perturbation,}
\]
\[
f_{\theta} \text{ be the target model with parameters } \theta,  
\]
\[
\text{and } \text{JPEG}(\cdot) \text{ represent the JPEG encoding function.}
\]

%Output
The output of the MaxJpeg Adversarial Attack is an adversarial example $x^*$ that is generated by applying perturbations to the original image $x$ and then encoding it using JPEG compression, while maximizing the quantization error.

%Formula
$x^* = \text{JPEG}(x + \delta)$

where:
- $\delta$ is the perturbation added to the input image $x$,
- $\text{JPEG}(\cdot)$ represents the JPEG encoding process.

The objective can be formulated as:
$\max_{\delta} \; \|x^* - x\|_p \quad \text{subject to} \quad f_\theta(x^*) \neq y$

%Explanation
The MaxJpeg Adversarial Attack is a variant of the original JPEG Adversarial Attack that maximizes the quantization error introduced by the JPEG encoding process. This is achieved by formulating the objective as maximizing the $L_p$ distance between the perturbed and original images, rather than minimizing it. By doing so, the attack introduces larger perturbations that result in a more significant impact on the model's output, making it harder to defend against. The addition of maximization instead of minimization allows the attack to generate adversarial examples with potentially higher accuracy, while maintaining the core principle of exploiting the vulnerabilities in image processing pipelines.