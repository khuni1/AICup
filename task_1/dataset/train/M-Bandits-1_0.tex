%Input
Classifier $L(x, y)$ for the image $x$ and true label $y$, latent vector $v_t$, search variance $\sigma$, number of samples $n$, perturbation bound $\epsilon$, query limit $L$, step size $\eta$, target class $y_{\text{adv}}$, dimensionality $d$, spherical gradient estimator $\Delta_t$, and Gaussian query vector $u$.

%Formula
The loss function for the bandit framework is:
$\ell_t(g) = -\left\langle \nabla L(x, y), \frac{g}{\|g\|} \right\rangle$
where $g$ is a gradient estimate accessed via finite differences. The spherical gradient estimate $\Delta_t$ is calculated as:
$\Delta = \frac{\ell(v + \delta u) - \ell(v - \delta u)}{\delta} u$
where $u$ is a Gaussian vector sampled from $\mathcal{N}(0, \frac{1}{d} I)$. The update rule for the latent vector $v_t$ in the bandit framework is then given by $v_t = v_{t-1} + \eta \cdot \Delta_t$
For the $\ell_\infty$ norm constraint, we employ the exponentiated gradients update, with the transformation $p_{t-1} = \frac{1}{2} (v_{t-1} + 1)$
$p_t = \frac{1}{Z} p_{t-1} \exp(\eta \cdot \Delta_t)$
where $Z = p_{t-1} \exp(\eta \cdot \Delta_t) + (1 - p_{t-1}) \exp(-\eta \cdot \Delta_t)$, and $v_t = 2p_t - 1$

%Output
The output is an adversarial image $x_{\text{adv}}$ that is misclassified by the classifier $L(x, y)$, and satisfies the given perturbation constraint (e.g., $\ell_2$ or $\ell_\infty$ norm).

%Explanation
The bandit framework for adversarial example generation is a general method for constructing black-box adversarial examples with any convex constraint set. The latent vector $v_t$ serves as a prior on the gradient for each round $t$. The inner product $\left\langle \nabla L(x, y), g \right\rangle$ represents the classification loss $L$ on image $x$ with label $y$, and $g$ is the gradient estimate for a given query. 

The bandit reduction process allows us to update the latent vector $v_t$ through first-order updates, using the gradient estimate $\Delta_t$. The gradient estimate is calculated using antithetic sampling, taking two queries of the classifier to compute a finite difference of the loss function. This is done efficiently using a spherical gradient estimator.

The choice of the update rule $A$ is crucial and depends on the constraint set $K$. For example, when $K = \mathbb{R}^n$, simple gradient ascent is used. However, for $\ell_\infty$ constraints, we employ an exponentiated gradients update. In this case, the updates involve transforming the latent vector into a probability distribution, applying the gradient update in this space, and then transforming back to the original space.

The attack aims to generate adversarial examples by iteratively updating the input image $x_t$ based on the estimated gradient $g_t$. In each iteration, the adversarial image is perturbed in the direction of the projected gradient, and the latent vector $v_t$ is updated based on the estimated gradient of the loss function. The process continues until the adversarial example $x_{\text{adv}}$ misleads the classifier into predicting the target class $y_{\text{adv}}$.


