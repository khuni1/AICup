%Input
$\pi$: The victim policy to be attacked.
$E$: Environment where the victim policy operates.
$\pi_{\text{adv}}$: Adversarial policy to train.
$T$: Total number of time steps in each episode.
$\mathcal{L}(\pi_{\text{adv}}, \pi)$: Loss function capturing the adversarial objective.
$\eta$: Learning rate for adversarial policy training.

%Output
Trained adversarial policy $\pi_{\text{adv}}$.

%Formula
1.Adversarial Policy Training:
   \[
   \pi_{\text{adv}}^* = \arg \max_{\pi_{\text{adv}}} \mathbb{E}_{\tau \sim E(\pi, \pi_{\text{adv}})} \Big[ R_{\text{adv}}(\tau) \Big],
   \]
   where $\tau = (\mathbf{s}_1, \mathbf{a}_1, \dots, \mathbf{s}_T, \mathbf{a}_T)$ is a trajectory generated by interacting policies $\pi$ and $\pi_{\text{adv}}$, and $R_{\text{adv}}$ is the adversarial reward.

2.Gradient Update:
   - Using gradient-based reinforcement learning, update $\pi_{\text{adv}}$:
     \[
     \pi_{\text{adv}} \leftarrow \pi_{\text{adv}} + \eta \nabla_{\pi_{\text{adv}}} \mathcal{L}(\pi_{\text{adv}}, \pi),
     \]
     where $\mathcal{L}$ is typically derived from the adversarial objective.

3.Loss Function:
   \[
   \mathcal{L}(\pi_{\text{adv}}, \pi) = -\mathbb{E}_{\tau \sim E(\pi, \pi_{\text{adv}})} \Big[ R_{\text{adv}}(\tau) \Big].
   \]

%Explanation
Training Adversarial Policies (TAP) can be developed following the below:
1.Objective:
   - TAP aims to train an adversarial policy $\pi_{\text{adv}}$ that interacts with a victim policy $\pi$ in a shared environment $E$ to degrade its performance.

2.Interaction:
   - Both policies interact within the same environment. The adversarial policy $\pi_{\text{adv}}$ exploits weaknesses in the victim's policy by selecting adversarial actions that disrupt the victim's objective.

3.Optimization:
   - The training of $\pi_{\text{adv}}$ uses gradient-based optimization methods such as policy gradient or actor-critic algorithms to maximize the adversarial reward $R_{\text{adv}}$.

4.Adversarial Reward:
   - The reward $R_{\text{adv}}$ is designed to evaluate the impact of the adversarial policy on the victim policy. For instance, $R_{\text{adv}}$ might correspond to minimizing the victim's total reward.

Key Characteristics:
- Target: Reinforcement learning policies.
- Effect: Reduces the victim's performance in a shared environment by training a competing adversarial policy.
- Type: Gradient-based adversarial training, leveraging reinforcement learning techniques.

